{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('cv_files/cv1_train.npy')\n",
    "valid = np.load('cv_files/cv1_valid.npy')\n",
    "column = np.load('cv_files/features_all_cv.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['row_id', 'user_id', 'content_id', 'content_type_id',\n",
       "       'answered_correctly', 'prior_question_elapsed_time',\n",
       "       'prior_question_had_explanation', 'answered_correctly_avg_c',\n",
       "       'answered_correctly_sum_u', 'count_u', 'answered_correctly_avg_u',\n",
       "       'prior_question_elapsed_time_mean', 'question_id', 'part'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = np.where(column == 'answered_correctly')[0][0]\n",
    "feature_name = ['answered_correctly_avg_u', 'answered_correctly_sum_u', 'count_u', 'answered_correctly_avg_c', 'part', 'prior_question_had_explanation', 'prior_question_elapsed_time']\n",
    "feature_col = np.array([np.where(column == col)[0][0] for col in feature_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([10,  8,  9,  7, 13,  6,  5], dtype=int64), 4)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "feature_col, target_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(train[:, feature_col], train[:, target_col])\n",
    "lgb_valid = lgb.Dataset(valid[:, feature_col], valid[:, target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Info] Number of positive: 63676135, number of negative: 33141405\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.329290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 96817540, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657692 -> initscore=0.653026\n",
      "[LightGBM] [Info] Start training from score 0.653026\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's binary_logloss: 0.548592\tvalid_1's binary_logloss: 0.55389\n",
      "[200]\ttraining's binary_logloss: 0.548165\tvalid_1's binary_logloss: 0.553372\n",
      "[300]\ttraining's binary_logloss: 0.547963\tvalid_1's binary_logloss: 0.553143\n",
      "[400]\ttraining's binary_logloss: 0.547843\tvalid_1's binary_logloss: 0.553021\n",
      "[500]\ttraining's binary_logloss: 0.547741\tvalid_1's binary_logloss: 0.552928\n",
      "[600]\ttraining's binary_logloss: 0.547655\tvalid_1's binary_logloss: 0.55285\n",
      "[700]\ttraining's binary_logloss: 0.547582\tvalid_1's binary_logloss: 0.552792\n",
      "[800]\ttraining's binary_logloss: 0.547513\tvalid_1's binary_logloss: 0.552728\n",
      "[900]\ttraining's binary_logloss: 0.547456\tvalid_1's binary_logloss: 0.552679\n",
      "[1000]\ttraining's binary_logloss: 0.547407\tvalid_1's binary_logloss: 0.552643\n",
      "[1100]\ttraining's binary_logloss: 0.547364\tvalid_1's binary_logloss: 0.552617\n",
      "[1200]\ttraining's binary_logloss: 0.547322\tvalid_1's binary_logloss: 0.552592\n",
      "[1300]\ttraining's binary_logloss: 0.547281\tvalid_1's binary_logloss: 0.55257\n",
      "[1400]\ttraining's binary_logloss: 0.547244\tvalid_1's binary_logloss: 0.552547\n",
      "[1500]\ttraining's binary_logloss: 0.547209\tvalid_1's binary_logloss: 0.552529\n",
      "Early stopping, best iteration is:\n",
      "[1492]\ttraining's binary_logloss: 0.547212\tvalid_1's binary_logloss: 0.552529\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(\n",
    "    {'objective': 'binary'}, \n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_valid],\n",
    "    verbose_eval=100,\n",
    "    num_boost_round=10000,\n",
    "    early_stopping_rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "auc: 0.7570404951690736\n"
     ]
    }
   ],
   "source": [
    "print('auc:', roc_auc_score(valid[:, target_col], model.predict(valid[:, feature_col])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x207e5565848>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "model.save_model('model/cv_process/sample.txt', num_iteration=model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(cv_idx):\n",
    "    train = np.load('cv_files/cv{0}_train.npy'.format(cv_idx))\n",
    "    valid = np.load('cv_files/cv{0}_valid.npy'.format(cv_idx))\n",
    "    return train, valid\n",
    "\n",
    "def get_dataset(cv_idx):\n",
    "    column = np.load('cv_files/features_all_cv.npy', allow_pickle=True)\n",
    "    target_col = np.where(column == 'answered_correctly')[0][0]\n",
    "    feature_name = ['answered_correctly_avg_u', 'answered_correctly_sum_u', 'count_u', 'answered_correctly_avg_c', 'part', 'prior_question_had_explanation', 'prior_question_elapsed_time']\n",
    "    feature_col = np.array([np.where(column == col)[0][0] for col in feature_name])\n",
    "    train, valid = read_data(cv_idx)\n",
    "    lgb_train = lgb.Dataset(train[:, feature_col], train[:, target_col])\n",
    "    lgb_valid = lgb.Dataset(valid[:, feature_col], valid[:, target_col])\n",
    "    return lgb_train, lgb_valid, valid[:, feature_col], valid[:, target_col]\n",
    "\n",
    "def create_model(trial):\n",
    "    num_leaves = trial.suggest_int('num_leaves', 26, 32)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 280, 350)\n",
    "    max_depth = trial.suggest_int('max_depth', 7, 9)\n",
    "    learning_rate = trial.suggest_uniform('learning_rate', 0.1, 0.5)\n",
    "    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 25, 90)\n",
    "    bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.1, 1.0)\n",
    "    feature_fraction = trial.suggest_uniform('feature_fraction', 0.1, 1.0)\n",
    "    model = lgb.train(\n",
    "        {\n",
    "            'objective': 'binary',\n",
    "            'num_leaves': num_leaves,\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'learning_rate': learning_rate,\n",
    "            'min_data_in_leaf': min_data_in_leaf,\n",
    "            'bagging_fraction': bagging_fraction,\n",
    "            'feature_fraction': feature_fraction,\n",
    "            'random_state': SEED\n",
    "        }, \n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_valid],\n",
    "        verbose_eval=-1,\n",
    "        num_boost_round=10000,\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    score = roc_auc_score(y_valid, model.predict(x_valid))\n",
    "    return score\n",
    "\n",
    "def train(cv_idx):\n",
    "    model = lgb.train(\n",
    "        params, \n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_valid],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=10000,\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    model.save_model('model/cv_process/baseline_cv{0}.txt'.format(cv_idx), num_iteration=model.best_iteration)\n",
    "    print('--- cv_idx : {0}, auc : {1}'.format(cv_idx, roc_auc_score(y_valid, model.predict(x_valid))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Info] Number of positive: 63676135, number of negative: 33141405\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.348825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 96817540, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657692 -> initscore=0.653026\n",
      "[LightGBM] [Info] Start training from score 0.653026\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's binary_logloss: 0.548573\tvalid_1's binary_logloss: 0.553858\n",
      "[200]\ttraining's binary_logloss: 0.548166\tvalid_1's binary_logloss: 0.553376\n",
      "[300]\ttraining's binary_logloss: 0.547969\tvalid_1's binary_logloss: 0.553161\n",
      "[400]\ttraining's binary_logloss: 0.547824\tvalid_1's binary_logloss: 0.553013\n",
      "[500]\ttraining's binary_logloss: 0.547726\tvalid_1's binary_logloss: 0.552917\n",
      "[600]\ttraining's binary_logloss: 0.547644\tvalid_1's binary_logloss: 0.552839\n",
      "[700]\ttraining's binary_logloss: 0.547573\tvalid_1's binary_logloss: 0.55278\n",
      "[800]\ttraining's binary_logloss: 0.547512\tvalid_1's binary_logloss: 0.552729\n",
      "Early stopping, best iteration is:\n",
      "[876]\ttraining's binary_logloss: 0.547467\tvalid_1's binary_logloss: 0.552698\n",
      "--- cv_idx : 1, auc : 0.7568434713971572\n",
      "[LightGBM] [Info] Number of positive: 62079865, number of negative: 32286996\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.373943 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 94366861, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657857 -> initscore=0.653757\n",
      "[LightGBM] [Info] Start training from score 0.653757\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's binary_logloss: 0.548562\tvalid_1's binary_logloss: 0.549145\n",
      "[200]\ttraining's binary_logloss: 0.548147\tvalid_1's binary_logloss: 0.54872\n",
      "[300]\ttraining's binary_logloss: 0.547955\tvalid_1's binary_logloss: 0.548547\n",
      "[400]\ttraining's binary_logloss: 0.547814\tvalid_1's binary_logloss: 0.548428\n",
      "[500]\ttraining's binary_logloss: 0.547704\tvalid_1's binary_logloss: 0.548353\n",
      "[600]\ttraining's binary_logloss: 0.547612\tvalid_1's binary_logloss: 0.548286\n",
      "[700]\ttraining's binary_logloss: 0.547541\tvalid_1's binary_logloss: 0.54824\n",
      "[800]\ttraining's binary_logloss: 0.547484\tvalid_1's binary_logloss: 0.548203\n",
      "Early stopping, best iteration is:\n",
      "[877]\ttraining's binary_logloss: 0.547441\tvalid_1's binary_logloss: 0.548176\n",
      "--- cv_idx : 2, auc : 0.7553841207073665\n",
      "[LightGBM] [Info] Number of positive: 60485727, number of negative: 31431203\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.407045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 91916930, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658048 -> initscore=0.654606\n",
      "[LightGBM] [Info] Start training from score 0.654606\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's binary_logloss: 0.548479\tvalid_1's binary_logloss: 0.551927\n",
      "[200]\ttraining's binary_logloss: 0.548083\tvalid_1's binary_logloss: 0.551585\n",
      "[300]\ttraining's binary_logloss: 0.547886\tvalid_1's binary_logloss: 0.551408\n",
      "[400]\ttraining's binary_logloss: 0.547748\tvalid_1's binary_logloss: 0.551298\n",
      "[500]\ttraining's binary_logloss: 0.547649\tvalid_1's binary_logloss: 0.551218\n",
      "[600]\ttraining's binary_logloss: 0.547564\tvalid_1's binary_logloss: 0.551158\n",
      "[700]\ttraining's binary_logloss: 0.547486\tvalid_1's binary_logloss: 0.551096\n",
      "[800]\ttraining's binary_logloss: 0.547429\tvalid_1's binary_logloss: 0.551056\n",
      "Early stopping, best iteration is:\n",
      "[831]\ttraining's binary_logloss: 0.54741\tvalid_1's binary_logloss: 0.551043\n",
      "--- cv_idx : 3, auc : 0.7521596792792301\n",
      "[LightGBM] [Info] Number of positive: 58891703, number of negative: 30575126\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.381584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 89466829, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658252 -> initscore=0.655513\n",
      "[LightGBM] [Info] Start training from score 0.655513\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's binary_logloss: 0.54846\tvalid_1's binary_logloss: 0.550567\n",
      "[200]\ttraining's binary_logloss: 0.548035\tvalid_1's binary_logloss: 0.550172\n",
      "[300]\ttraining's binary_logloss: 0.547848\tvalid_1's binary_logloss: 0.550013\n",
      "[400]\ttraining's binary_logloss: 0.547713\tvalid_1's binary_logloss: 0.5499\n",
      "[500]\ttraining's binary_logloss: 0.547602\tvalid_1's binary_logloss: 0.549816\n",
      "[600]\ttraining's binary_logloss: 0.547516\tvalid_1's binary_logloss: 0.549753\n",
      "[700]\ttraining's binary_logloss: 0.547439\tvalid_1's binary_logloss: 0.5497\n",
      "[800]\ttraining's binary_logloss: 0.547376\tvalid_1's binary_logloss: 0.549661\n",
      "[900]\ttraining's binary_logloss: 0.547317\tvalid_1's binary_logloss: 0.549625\n",
      "[1000]\ttraining's binary_logloss: 0.547272\tvalid_1's binary_logloss: 0.549602\n",
      "Early stopping, best iteration is:\n",
      "[1021]\ttraining's binary_logloss: 0.547263\tvalid_1's binary_logloss: 0.549598\n",
      "--- cv_idx : 4, auc : 0.7543035379017176\n",
      "[LightGBM] [Info] Number of positive: 57291408, number of negative: 29725656\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.371194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 87017064, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658393 -> initscore=0.656140\n",
      "[LightGBM] [Info] Start training from score 0.656140\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's binary_logloss: 0.548392\tvalid_1's binary_logloss: 0.550068\n",
      "[200]\ttraining's binary_logloss: 0.547985\tvalid_1's binary_logloss: 0.549721\n",
      "[300]\ttraining's binary_logloss: 0.547801\tvalid_1's binary_logloss: 0.549569\n",
      "[400]\ttraining's binary_logloss: 0.547662\tvalid_1's binary_logloss: 0.549463\n",
      "[500]\ttraining's binary_logloss: 0.547551\tvalid_1's binary_logloss: 0.549381\n",
      "[600]\ttraining's binary_logloss: 0.547472\tvalid_1's binary_logloss: 0.549323\n",
      "[700]\ttraining's binary_logloss: 0.547398\tvalid_1's binary_logloss: 0.549276\n",
      "[800]\ttraining's binary_logloss: 0.547331\tvalid_1's binary_logloss: 0.549234\n",
      "[900]\ttraining's binary_logloss: 0.547274\tvalid_1's binary_logloss: 0.549194\n",
      "[1000]\ttraining's binary_logloss: 0.547216\tvalid_1's binary_logloss: 0.54916\n",
      "[1100]\ttraining's binary_logloss: 0.54717\tvalid_1's binary_logloss: 0.549133\n",
      "[1200]\ttraining's binary_logloss: 0.547115\tvalid_1's binary_logloss: 0.5491\n",
      "[1300]\ttraining's binary_logloss: 0.547073\tvalid_1's binary_logloss: 0.549078\n",
      "Early stopping, best iteration is:\n",
      "[1334]\ttraining's binary_logloss: 0.54706\tvalid_1's binary_logloss: 0.549073\n",
      "--- cv_idx : 5, auc : 0.7529376948691685\n"
     ]
    }
   ],
   "source": [
    "optimize = False\n",
    "for cv_idx in np.arange(1, 6):\n",
    "    sampler = TPESampler(seed=SEED)\n",
    "    lgb_train, lgb_valid, x_valid, y_valid = get_dataset(cv_idx)\n",
    "    if optimize:\n",
    "        optim = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "        optim.optimize(objective, n_trials=50)\n",
    "        params = optim.best_params\n",
    "    else:\n",
    "        params = dict()\n",
    "    params['objective'] = 'binary'\n",
    "    params['random_seed'] = SEED\n",
    "    train(cv_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}