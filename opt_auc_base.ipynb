{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('cv_files/cv1_train.npy')\n",
    "valid = np.load('cv_files/cv1_valid.npy')\n",
    "column = np.load('cv_files/features_all_cv.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['row_id', 'user_id', 'content_id', 'content_type_id',\n",
       "       'answered_correctly', 'prior_question_elapsed_time',\n",
       "       'prior_question_had_explanation', 'answered_correctly_avg_c',\n",
       "       'answered_correctly_sum_u', 'count_u', 'answered_correctly_avg_u',\n",
       "       'prior_question_elapsed_time_mean', 'question_id', 'part'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = np.where(column == 'answered_correctly')[0][0]\n",
    "feature_name = ['answered_correctly_avg_u', 'answered_correctly_sum_u', 'count_u', 'answered_correctly_avg_c', 'part', 'prior_question_had_explanation', 'prior_question_elapsed_time']\n",
    "feature_col = np.array([np.where(column == col)[0][0] for col in feature_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([10,  8,  9,  7, 13,  6,  5], dtype=int64), 4)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "feature_col, target_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(train[:, feature_col], train[:, target_col])\n",
    "lgb_valid = lgb.Dataset(valid[:, feature_col], valid[:, target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Info] Number of positive: 63676135, number of negative: 33141405\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.384281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 96817540, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.657692 -> initscore=0.653026\n",
      "[LightGBM] [Info] Start training from score 0.653026\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's binary_logloss: 0.548592\tvalid_1's binary_logloss: 0.55389\n",
      "[200]\ttraining's binary_logloss: 0.548165\tvalid_1's binary_logloss: 0.553372\n",
      "[300]\ttraining's binary_logloss: 0.547963\tvalid_1's binary_logloss: 0.553143\n",
      "[400]\ttraining's binary_logloss: 0.547843\tvalid_1's binary_logloss: 0.553021\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-96fedf500a39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;32m~\\anaconda_rap\\anaconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalid_sets\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m                 \u001b[0mevaluation_result_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m             \u001b[0mevaluation_result_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda_rap\\anaconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36meval_train\u001b[1;34m(self, feval)\u001b[0m\n\u001b[0;32m   2581\u001b[0m             \u001b[0mList\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2582\u001b[0m         \"\"\"\n\u001b[1;32m-> 2583\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__inner_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_data_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2585\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0meval_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda_rap\\anaconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__inner_eval\u001b[1;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[0;32m   3106\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3107\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_out_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3108\u001b[1;33m                 result.ctypes.data_as(ctypes.POINTER(ctypes.c_double))))\n\u001b[0m\u001b[0;32m   3109\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtmp_out_len\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_inner_eval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3110\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Wrong length of eval results\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = lgb.train(\n",
    "    {'objective': 'binary'}, \n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_valid],\n",
    "    verbose_eval=100,\n",
    "    num_boost_round=10000,\n",
    "    early_stopping_rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "auc: 0.7570404951690736\n"
     ]
    }
   ],
   "source": [
    "print('auc:', roc_auc_score(valid[:, target_col], model.predict(valid[:, feature_col])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x28718fadec8>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "model.save_model('model/cv_process/sample.txt', num_iteration=model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(cv_idx):\n",
    "    train = np.load('cv_files/cv{0}_train.npy'.format(cv_idx))\n",
    "    valid = np.load('cv_files/cv{0}_valid.npy'.format(cv_idx))\n",
    "    return train, valid\n",
    "\n",
    "def get_dataset(cv_idx):\n",
    "    column = np.load('cv_files/features_all_cv.npy', allow_pickle=True)\n",
    "    target_col = np.where(column == 'answered_correctly')[0][0]\n",
    "    feature_name = ['answered_correctly_avg_u', 'answered_correctly_sum_u', 'count_u', 'answered_correctly_avg_c', 'part', 'prior_question_had_explanation', 'prior_question_elapsed_time']\n",
    "    feature_col = np.array([np.where(column == col)[0][0] for col in feature_name])\n",
    "    train, valid = read_data(cv_idx)\n",
    "    lgb_train = lgb.Dataset(train[:, feature_col], train[:, target_col])\n",
    "    lgb_valid = lgb.Dataset(valid[:, feature_col], valid[:, target_col])\n",
    "    return lgb_train, lgb_valid, valid[:, feature_col], valid[:, target_col]\n",
    "\n",
    "def create_model(trial):\n",
    "    num_leaves = trial.suggest_int('num_leaves', 26, 32)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 280, 350)\n",
    "    max_depth = trial.suggest_int('max_depth', 7, 9)\n",
    "    learning_rate = trial.suggest_uniform('learning_rate', 0.1, 0.5)\n",
    "    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 25, 90)\n",
    "    bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.1, 1.0)\n",
    "    feature_fraction = trial.suggest_uniform('feature_fraction', 0.1, 1.0)\n",
    "    model = lgb.train(\n",
    "        {\n",
    "            'objective': 'binary',\n",
    "            'num_leaves': num_leaves,\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'learning_rate': learning_rate,\n",
    "            'min_data_in_leaf': min_data_in_leaf,\n",
    "            'bagging_fraction': bagging_fraction,\n",
    "            'feature_fraction': feature_fraction,\n",
    "            'feature_pre_filter': False,\n",
    "            'random_state': SEED\n",
    "        }, \n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_valid],\n",
    "        verbose_eval=-1,\n",
    "        num_boost_round=10000,\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    score = roc_auc_score(y_valid, model.predict(x_valid))\n",
    "    return score\n",
    "\n",
    "def train(cv_idx):\n",
    "    model = lgb.train(\n",
    "        params, \n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_valid],\n",
    "        verbose_eval=100,\n",
    "        num_boost_round=10000,\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    model.save_model('model/cv_process/opt_auc_base_cv{0}.txt'.format(cv_idx), num_iteration=model.best_iteration)\n",
    "    print('--- cv_idx : {0}, auc : {1}'.format(cv_idx, roc_auc_score(y_valid, model.predict(x_valid))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ghtGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Info] Number of positive: 57291408, number of negative: 29725656\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.606143 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 87017064, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658393 -> initscore=0.656140\n",
      "[LightGBM] [Info] Start training from score 0.656140\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[339]\ttraining's binary_logloss: 0.547253\tvalid_1's binary_logloss: 0.549234\n",
      "\u001b[32m[I 2021-01-07 17:23:07,433]\u001b[0m Trial 84 finished with value: 0.7527450276948352 and parameters: {'num_leaves': 31, 'n_estimators': 339, 'max_depth': 7, 'learning_rate': 0.44164599744370225, 'min_data_in_leaf': 74, 'bagging_fraction': 0.5857964154703156, 'feature_fraction': 0.9353317392709618}. Best is trial 19 with value: 0.7527972849958232.\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 57291408, number of negative: 29725656\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.558465 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 87017064, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658393 -> initscore=0.656140\n",
      "[LightGBM] [Info] Start training from score 0.656140\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[346]\ttraining's binary_logloss: 0.54724\tvalid_1's binary_logloss: 0.549232\n",
      "\u001b[32m[I 2021-01-07 17:30:20,281]\u001b[0m Trial 85 finished with value: 0.7527436690870108 and parameters: {'num_leaves': 32, 'n_estimators': 346, 'max_depth': 7, 'learning_rate': 0.4132537193265257, 'min_data_in_leaf': 88, 'bagging_fraction': 0.4655737066636628, 'feature_fraction': 0.9650624811229966}. Best is trial 19 with value: 0.7527972849958232.\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 57291408, number of negative: 29725656\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.429580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 87017064, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658393 -> initscore=0.656140\n",
      "[LightGBM] [Info] Start training from score 0.656140\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[348]\ttraining's binary_logloss: 0.547313\tvalid_1's binary_logloss: 0.54929\n",
      "\u001b[32m[I 2021-01-07 17:38:44,494]\u001b[0m Trial 86 finished with value: 0.75270037126172 and parameters: {'num_leaves': 28, 'n_estimators': 348, 'max_depth': 7, 'learning_rate': 0.45435640389090703, 'min_data_in_leaf': 68, 'bagging_fraction': 0.5973805588044236, 'feature_fraction': 0.8983203806527523}. Best is trial 19 with value: 0.7527972849958232.\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 57291408, number of negative: 29725656\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.546849 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 87017064, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658393 -> initscore=0.656140\n",
      "[LightGBM] [Info] Start training from score 0.656140\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[286]\ttraining's binary_logloss: 0.547364\tvalid_1's binary_logloss: 0.5493\n",
      "\u001b[32m[I 2021-01-07 17:46:13,170]\u001b[0m Trial 87 finished with value: 0.7526853431084742 and parameters: {'num_leaves': 32, 'n_estimators': 341, 'max_depth': 7, 'learning_rate': 0.37876779555898665, 'min_data_in_leaf': 86, 'bagging_fraction': 0.4985142494555954, 'feature_fraction': 0.9492056716310041}. Best is trial 19 with value: 0.7527972849958232.\u001b[0m\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Info] Number of positive: 57291408, number of negative: 29725656\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.536713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 87017064, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658393 -> initscore=0.656140\n",
      "[LightGBM] [Info] Start training from score 0.656140\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[334]\ttraining's binary_logloss: 0.547212\tvalid_1's binary_logloss: 0.549275\n",
      "\u001b[32m[I 2021-01-07 17:54:25,774]\u001b[0m Trial 88 finished with value: 0.7527190086742999 and parameters: {'num_leaves': 31, 'n_estimators': 334, 'max_depth': 7, 'learning_rate': 0.48765572289486697, 'min_data_in_leaf': 71, 'bagging_fraction': 0.3705312179101413, 'feature_fraction': 0.9822378764829494}. Best is trial 19 with value: 0.7527972849958232.\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 57291408, number of negative: 29725656\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.536052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 87017064, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658393 -> initscore=0.656140\n",
      "[LightGBM] [Info] Start training from score 0.656140\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[331]\ttraining's binary_logloss: 0.547249\tvalid_1's binary_logloss: 0.549278\n",
      "\u001b[32m[I 2021-01-07 18:02:30,722]\u001b[0m Trial 89 finished with value: 0.7526860298674992 and parameters: {'num_leaves': 30, 'n_estimators': 331, 'max_depth': 8, 'learning_rate': 0.47905959176386625, 'min_data_in_leaf': 80, 'bagging_fraction': 0.4337403356471216, 'feature_fraction': 0.8069687940604479}. Best is trial 19 with value: 0.7527972849958232.\u001b[0m\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Info] Number of positive: 57291408, number of negative: 29725656\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.523514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 87017064, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658393 -> initscore=0.656140\n",
      "[LightGBM] [Info] Start training from score 0.656140\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[282]\ttraining's binary_logloss: 0.547366\tvalid_1's binary_logloss: 0.549319\n",
      "\u001b[32m[I 2021-01-07 18:09:51,912]\u001b[0m Trial 90 finished with value: 0.7526552332251392 and parameters: {'num_leaves': 31, 'n_estimators': 350, 'max_depth': 7, 'learning_rate': 0.4647570598852956, 'min_data_in_leaf': 82, 'bagging_fraction': 0.6743958960082936, 'feature_fraction': 0.9204894030211276}. Best is trial 19 with value: 0.7527972849958232.\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 57291408, number of negative: 29725656\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.541059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 87017064, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658393 -> initscore=0.656140\n",
      "[LightGBM] [Info] Start training from score 0.656140\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[345]\ttraining's binary_logloss: 0.547228\tvalid_1's binary_logloss: 0.549266\n",
      "\u001b[32m[I 2021-01-07 18:18:19,866]\u001b[0m Trial 91 finished with value: 0.7527170392051282 and parameters: {'num_leaves': 32, 'n_estimators': 345, 'max_depth': 7, 'learning_rate': 0.42785759025037307, 'min_data_in_leaf': 84, 'bagging_fraction': 0.5363441513187265, 'feature_fraction': 0.9992793480174457}. Best is trial 19 with value: 0.7527972849958232.\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 57291408, number of negative: 29725656\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.547342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 87017064, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658393 -> initscore=0.656140\n",
      "[LightGBM] [Info] Start training from score 0.656140\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[343]\ttraining's binary_logloss: 0.547232\tvalid_1's binary_logloss: 0.549237\n",
      "\u001b[32m[I 2021-01-07 18:26:48,101]\u001b[0m Trial 92 finished with value: 0.7527449809603354 and parameters: {'num_leaves': 32, 'n_estimators': 343, 'max_depth': 7, 'learning_rate': 0.4318464172841304, 'min_data_in_leaf': 76, 'bagging_fraction': 0.6329824876166626, 'feature_fraction': 0.9823927719303799}. Best is trial 19 with value: 0.7527972849958232.\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 57291408, number of negative: 29725656\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.568576 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 87017064, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658393 -> initscore=0.656140\n",
      "[LightGBM] [Info] Start training from score 0.656140\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[311]\ttraining's binary_logloss: 0.547307\tvalid_1's binary_logloss: 0.549281\n",
      "\u001b[32m[I 2021-01-07 18:34:59,740]\u001b[0m Trial 93 finished with value: 0.7527119460432259 and parameters: {'num_leaves': 32, 'n_estimators': 340, 'max_depth': 7, 'learning_rate': 0.39112006272693745, 'min_data_in_leaf': 89, 'bagging_fraction': 0.5101968233667838, 'feature_fraction': 0.9552204737887809}. Best is trial 19 with value: 0.7527972849958232.\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 57291408, number of negative: 29725656\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.534014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 87017064, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658393 -> initscore=0.656140\n",
      "[LightGBM] [Info] Start training from score 0.656140\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[318]\ttraining's binary_logloss: 0.547282\tvalid_1's binary_logloss: 0.549299\n",
      "\u001b[32m[I 2021-01-07 18:43:08,886]\u001b[0m Trial 94 finished with value: 0.7526613225227754 and parameters: {'num_leaves': 32, 'n_estimators': 337, 'max_depth': 7, 'learning_rate': 0.45358134613700773, 'min_data_in_leaf': 79, 'bagging_fraction': 0.2293311340362399, 'feature_fraction': 0.8783079195059504}. Best is trial 19 with value: 0.7527972849958232.\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 57291408, number of negative: 29725656\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.540836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 87017064, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658393 -> initscore=0.656140\n",
      "[LightGBM] [Info] Start training from score 0.656140\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[347]\ttraining's binary_logloss: 0.547367\tvalid_1's binary_logloss: 0.549284\n",
      "\u001b[32m[I 2021-01-07 18:51:54,071]\u001b[0m Trial 95 finished with value: 0.7526929045274005 and parameters: {'num_leaves': 32, 'n_estimators': 347, 'max_depth': 7, 'learning_rate': 0.2768847584924767, 'min_data_in_leaf': 63, 'bagging_fraction': 0.17533999519105894, 'feature_fraction': 0.9828148127046307}. Best is trial 19 with value: 0.7527972849958232.\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 57291408, number of negative: 29725656\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.537247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 87017064, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658393 -> initscore=0.656140\n",
      "[LightGBM] [Info] Start training from score 0.656140\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[253]\ttraining's binary_logloss: 0.547426\tvalid_1's binary_logloss: 0.549328\n",
      "\u001b[32m[I 2021-01-07 18:56:53,652]\u001b[0m Trial 96 finished with value: 0.7526413547294764 and parameters: {'num_leaves': 27, 'n_estimators': 339, 'max_depth': 7, 'learning_rate': 0.49275486452334594, 'min_data_in_leaf': 87, 'bagging_fraction': 0.4655650243556423, 'feature_fraction': 0.9379412124067288}. Best is trial 19 with value: 0.7527972849958232.\u001b[0m\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Info] Number of positive: 57291408, number of negative: 29725656\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.413994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 87017064, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658393 -> initscore=0.656140\n",
      "[LightGBM] [Info] Start training from score 0.656140\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[342]\ttraining's binary_logloss: 0.547261\tvalid_1's binary_logloss: 0.549253\n",
      "\u001b[32m[I 2021-01-07 19:03:12,400]\u001b[0m Trial 97 finished with value: 0.7527329115288247 and parameters: {'num_leaves': 31, 'n_estimators': 342, 'max_depth': 7, 'learning_rate': 0.44176457031297267, 'min_data_in_leaf': 85, 'bagging_fraction': 0.2752857533013799, 'feature_fraction': 0.9088497834444742}. Best is trial 19 with value: 0.7527972849958232.\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 57291408, number of negative: 29725656\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.418281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 87017064, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658393 -> initscore=0.656140\n",
      "[LightGBM] [Info] Start training from score 0.656140\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[345]\ttraining's binary_logloss: 0.547237\tvalid_1's binary_logloss: 0.54922\n",
      "\u001b[32m[I 2021-01-07 19:09:32,868]\u001b[0m Trial 98 finished with value: 0.7527707413860282 and parameters: {'num_leaves': 32, 'n_estimators': 345, 'max_depth': 7, 'learning_rate': 0.41535682787032346, 'min_data_in_leaf': 71, 'bagging_fraction': 0.2156447793722495, 'feature_fraction': 0.9609000171047498}. Best is trial 19 with value: 0.7527972849958232.\u001b[0m\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Info] Number of positive: 57291408, number of negative: 29725656\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.463173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 87017064, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658393 -> initscore=0.656140\n",
      "[LightGBM] [Info] Start training from score 0.656140\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[346]\ttraining's binary_logloss: 0.547245\tvalid_1's binary_logloss: 0.549257\n",
      "\u001b[32m[I 2021-01-07 19:16:58,077]\u001b[0m Trial 99 finished with value: 0.7527330109184392 and parameters: {'num_leaves': 31, 'n_estimators': 346, 'max_depth': 7, 'learning_rate': 0.41950700540637476, 'min_data_in_leaf': 71, 'bagging_fraction': 0.21372388491192226, 'feature_fraction': 0.9604078254896471}. Best is trial 19 with value: 0.7527972849958232.\u001b[0m\n",
      "[LightGBM] [Info] Number of positive: 57291408, number of negative: 29725656\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.507190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 87017064, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.658393 -> initscore=0.656140\n",
      "[LightGBM] [Info] Start training from score 0.656140\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[100]\ttraining's auc: 0.751008\tvalid_1's auc: 0.752247\n",
      "[200]\ttraining's auc: 0.751422\tvalid_1's auc: 0.752558\n",
      "[300]\ttraining's auc: 0.751704\tvalid_1's auc: 0.752745\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[341]\ttraining's auc: 0.751791\tvalid_1's auc: 0.752797\n",
      "--- cv_idx : 5, auc : 0.7527972849958232\n"
     ]
    }
   ],
   "source": [
    "optimize = True\n",
    "for cv_idx in np.arange(1, 6):\n",
    "    sampler = TPESampler(seed=SEED)\n",
    "    lgb_train, lgb_valid, x_valid, y_valid = get_dataset(cv_idx)\n",
    "    if optimize:\n",
    "        optim = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "        optim.optimize(objective, n_trials=100)\n",
    "        params = optim.best_params\n",
    "    else:\n",
    "        params = dict()\n",
    "    params['objective'] = 'binary'\n",
    "    params['metric'] = 'auc'\n",
    "    params['random_seed'] = SEED\n",
    "    train(cv_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}